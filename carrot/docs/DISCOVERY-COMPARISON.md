# Discovery System - Old vs New Comparison

## ğŸ“Š **What We Have (Working)**

### **File**: `carrot/src/app/api/patches/[handle]/start-discovery/route.ts`

**Status**: âœ… **WORKING IN PRODUCTION**

**What it does:**
1. âœ… Calls DeepSeek to find 10+ relevant items
2. âœ… Filters by relevance score (>0.7)
3. âœ… Generates AI images for each item
4. âœ… Saves to database with status: 'ready'
5. âœ… Streams via SSE (item-ready events)
6. âœ… Tracks rejected items

**What it's missing:**
- âŒ No duplicate URL checking (just inserts everything from DeepSeek)
- âŒ No content hash fingerprinting (can insert near-duplicates)
- âŒ Logs every duplicate skip individually (console spam)
- âŒ No metrics tracking
- âŒ Fetches ALL items at once (batch mode, not one-at-a-time)

---

## ğŸ†• **What We Built (New System)**

### **Files**: `carrot/src/lib/discovery/*`

**Status**: ğŸš§ **NEW, UNTESTED**

**What it adds:**
1. âœ… **Canonicalization** - URL normalization
2. âœ… **Multi-tier deduplication** - URL + Content Hash + Title Similarity
3. âœ… **Batched logging** - Reduces spam by >90%
4. âœ… **Metrics tracking** - Performance monitoring
5. âœ… **Search frontier** - Priority-based source selection
6. âœ… **One-at-a-time** - Single DeepSeek call per iteration
7. âœ… **Redis caching** - Fast duplicate checks

**Issues:**
- âš ï¸ Completely new architecture (high debug risk)
- âš ï¸ Requires Redis (new dependency)
- âš ï¸ Needs database migration
- âš ï¸ Untested with real data
- âš ï¸ More complex to troubleshoot

---

## ğŸ¯ **Recommended Approach: Enhance Existing System**

### **Strategy**: Keep what works, add only what's needed

### **Phase 1: Add Duplicate Detection** (Low Risk)
Add to existing `start-discovery/route.ts`:

```typescript
// BEFORE saving to database (line ~246)

// STEP 2.5: Check for duplicates
const existing = await prisma.discoveredContent.findFirst({
  where: {
    patchId: patch.id,
    OR: [
      { sourceUrl: item.url },
      { canonicalUrl: item.url }
    ]
  }
})

if (existing) {
  console.log(`[Discovery] Skipping duplicate: ${item.url}`)
  duplicateCount++
  continue
}
```

**Benefits:**
- âœ… Prevents duplicate inserts
- âœ… 5 lines of code
- âœ… No new dependencies
- âœ… Low risk

### **Phase 2: Add Batched Logging** (Low Risk)
Replace individual `console.log` with batched logger:

```typescript
// At top of file
import { BatchedLogger } from '@/lib/discovery/logger'
const logger = new BatchedLogger(60000)

// Replace all duplicate logs
logger.logDuplicate(item.url, 'A', 'deepseek')

// At end of processing
logger.flush()
```

**Benefits:**
- âœ… Reduces console spam by >90%
- âœ… Reuses new logger code
- âœ… No architectural changes

### **Phase 3: Add URL Canonicalization** (Medium Risk)
Use the canonicalization utility:

```typescript
// BEFORE duplicate check
import { canonicalize } from '@/lib/discovery/canonicalization'

const canonicalResult = await canonicalize(item.url)
const canonicalUrl = canonicalResult.canonicalUrl

// Then save canonicalUrl to database
```

**Benefits:**
- âœ… Better duplicate detection
- âœ… Reuses new canonicalization code
- âœ… Minimal changes to existing flow

### **Phase 4: Add Content Fingerprinting** (Optional, Higher Risk)
Only if we see near-duplicate issues:

```typescript
import { generateContentFingerprint, isNearDuplicate } from '@/lib/discovery/simhash'

const contentHash = generateContentFingerprint({
  title: item.title,
  content: item.description
})

// Check against recent hashes
const recentHashes = await prisma.discoveredContent.findMany({
  where: { patchId: patch.id },
  select: { contentHash: true },
  take: 100,
  orderBy: { createdAt: 'desc' }
})

for (const existing of recentHashes) {
  if (existing.contentHash && isNearDuplicate(contentHash, existing.contentHash, 3)) {
    console.log('[Discovery] Skipping near-duplicate content')
    continue outerLoop
  }
}
```

---

## ğŸ” **Reusable Components from New System**

### **Keep These (Standalone Utilities):**
1. âœ… **`canonicalization.ts`** - Pure function, no dependencies
2. âœ… **`simhash.ts`** - Pure function, no dependencies
3. âœ… **`logger.ts`** - Batched logging, useful everywhere

### **Maybe Keep (If We Need Them Later):**
4. ğŸ¤” **`deduplication.ts`** - Good logic, but needs Redis
5. ğŸ¤” **`redis.ts`** - Useful for caching, but adds dependency
6. ğŸ¤” **`providers.ts`** - Provider optimization, may need later

### **Delete These (Over-engineered):**
7. âŒ **`frontier.ts`** - Too complex for current needs
8. âŒ **`discovery-loop.ts`** - Duplicates existing working code
9. âŒ **`sse.ts`** - Already have SSE in start-discovery route
10. âŒ **`migration.ts`** - Won't need if we enhance existing schema

---

## âœ… **Action Plan**

### **Immediate (This Session):**
1. Review `canonicalization.ts`, `simhash.ts`, `logger.ts` - these are safe
2. Enhance existing `start-discovery/route.ts` with:
   - Duplicate URL checking
   - Batched logging
   - Optional: URL canonicalization
3. Test locally to ensure it works
4. Push to git
5. Delete unused new files

### **Future Enhancements:**
- Add content fingerprinting if near-duplicates become an issue
- Add Redis caching if database queries become slow
- Add metrics tracking for monitoring
- Add search frontier if we need multi-source rotation

---

## ğŸ¯ **Final Recommendation**

**Use the hybrid approach:**
1. **Keep working code** (`start-discovery/route.ts`)
2. **Add 3 proven utilities** (canonicalization, simhash, logger)
3. **Enhance with simple duplicate check**
4. **Test and iterate**

**Result:**
- âœ… Low risk (builds on working code)
- âœ… Fast to implement (5-10 lines of changes)
- âœ… Easy to debug (minimal new code)
- âœ… Solves actual problems (duplicates, log spam)
- âœ… Keeps new utilities for future use

**Time estimate:** 15 minutes to enhance, 5 minutes to test, 5 minutes to deploy

vs.

**New system:** 2-4 hours to debug, unknown time to production-ready

---

**Verdict**: Enhance existing system with selective features from new code. ğŸ¯
