# Discovery Process Flow

## When You Click "Start Discovery"

### 1. **Initial Setup** (`/api/patches/[handle]/start-discovery`)
   - Creates a `DiscoveryRun` record
   - Generates a `DiscoveryPlan` using DeepSeek (search strategy)
   - Creates a `GuideSnapshot` (AI-generated guide)
   - Seeds the `Frontier` (Redis queue) with initial URLs from the plan
   - Calls `runOpenEvidenceEngine()` to start the engine

### 2. **Discovery Engine Starts** (`engineV21.ts`)
   - Loads the discovery plan from Redis
   - Initializes scheduler guards (rate limiting, host caps)
   - Starts heartbeat monitoring
   - Begins the main discovery loop

### 3. **Main Discovery Loop** (Runs continuously)

   **A. Priority Burst (First 3 items)**
   - Processes top 3 priority candidates immediately
   - These are usually the most important seed URLs

   **B. Main Loop (Every iteration)**
   
   **Step 1: Pull Candidate from Frontier**
   - Gets next URL from Redis frontier queue
   - Applies bias (controversy, host diversity, etc.)
   - Checks scheduler guards (rate limits, host caps)
   
   **Step 2: Process Candidate**
   - **Fetch**: Downloads the URL (with timeout, paywall handling)
   - **Extract**: Extracts HTML content and converts to text
   - **Vet**: Uses AI to analyze content quality and relevance
   - **Accept**: Checks if content meets thresholds (relevance, quality, importance)
   - **Deduplicate**: Checks for duplicates/near-duplicates
   - **Save**: If passes all checks, saves to `DiscoveredContent`
   - **Hero**: Triggers hero image generation (background)
   
   **Step 3: Expand Frontier** (if needed)
   - If frontier is empty, expands with new URLs:
     - Query expansion (generates new search queries)
     - Deep link extraction (extracts links from processed pages)
     - Wikipedia references (extracts citations from Wikipedia pages)
   - Adds new candidates to frontier for processing

   **C. Wikipedia Processing (Every 30 seconds or every 10 candidates)**
   - Calls `processWikipediaIncremental()`
   - Processes 1 Wikipedia page and up to 50 citations
   - For each citation:
     - Verifies URL is accessible
     - Fetches and extracts content
     - Scores with DeepSeek (relevance check, score >= 60)
     - If relevant: Saves to `DiscoveredContent`
     - If relevant: Saves to `AgentMemory` (for AI knowledge)
     - Marks citation as scanned with decision

### 4. **What Gets Searched**

   **Initial Seeds** (from Discovery Plan):
   - Wikipedia pages related to the topic
   - Official sources (government sites, organizations)
   - News sources
   - Academic sources
   - Query-generated URLs (from DeepSeek search queries)

   **Expanded Sources** (during discovery):
   - Links extracted from processed pages
   - Wikipedia citations from monitored pages
   - New search queries generated by query expansion
   - Deep links from relevant domains

### 5. **Processing Order**

   1. **Priority candidates** (first 3) - immediate
   2. **Frontier queue** - processed in order with bias:
      - Controversy candidates prioritized early
      - Host diversity maintained
      - Wikipedia share limited (max 30%)
   3. **Wikipedia citations** - processed incrementally (every 30s)

### 6. **What Gets Saved**

   **To `DiscoveredContent`** (if passes all checks):
   - Relevance score >= 0.65
   - Quality score >= 70
   - Importance score >= 50
   - Not a duplicate
   - Has sufficient content (>= 450 chars)
   - Passes vetter checks

   **To `AgentMemory`** (if relevant):
   - Wikipedia citations that score >= 60
   - Tagged by Wikipedia page for organization

### 7. **Stopping Conditions**

   - Frontier empty AND no expansion possible
   - Run cap reached (max attempts)
   - User stops discovery
   - Zero-save SLO triggered (too many failures)
   - Timeout reached

## Current State for Israel Patch

- **8,827 external URLs identified** (from Wikipedia pages)
- **149 processed** (1.7%)
- **0 saved** (all scored below 60 threshold)
- **8,670 pending** (not yet processed)

The system will process these 8,670 pending citations as discovery runs, checking each one for relevance and saving those that score >= 60.

